[**ä¸­æ–‡**](./README_zh.md) | [**English**](./README.md)

<div id="top"></div>

# Spider & Entropy

The repo is used to calculate the entropy of text data, and provides functions for crawlers to obtain text files and data processing.

The example data are crawled from China News Network, English wikipedia and Chinese wikipedia, and Ancient Poetry Network. A certain degree of text cleaning was performed on the Chinese and English texts respectively.

And use jieba to Chinese participle, spacy to English lexical reduction, opencc traditional to simplified.

## Dependencies

Before getting started, make sure you have the following dependencies installed:

- Python 3.x
- Install other Python libraries (see the `requirements.txt` file)

You can install Python libraries using the following command:

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

## ğŸ“ Folder

```bash
â”œâ”€â”€ data/ # Crawled data
â”‚   â”œâ”€â”€news_zh # Chinese news network 
â”‚   â”œâ”€â”€wiki_zh # Chinese wikipedia
â”‚   â”œâ”€â”€wiki_en # English wikipedia
â”œâ”€â”€ photos/ # diagrams
â”œâ”€â”€ get_data_en # Crawl English wikipedia
â”œâ”€â”€ get_data_zh # Crawl Chinese wikipedia
â”œâ”€â”€ get_news # Crawl Chinese wikipedia
â”œâ”€â”€ get_gushiwen # Crawl ancient Chinese wikipedia
â”œâ”€â”€ main # Calculating entropy, plotting graphs, verifying Ziff's law.
â”œâ”€â”€ process_data # Process data, text cleaning, split words, etc.
```

## ğŸ”§ Get started

### QuickStart

run to calculate the entropy and draw diagrams.

```bash
python -u main.py
```

### Data

Crawl the data according to the get file itself, the crawled data is stored in the data folder.
